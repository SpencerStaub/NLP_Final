{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Design a Quiz\n",
      "Which book would you like to use?\n",
      "Here are your choices:\n",
      "\n",
      "The Best Bad Luck I Ever Had \n",
      " The Lions of Little Rock\n",
      " The Paper Cowboy\n",
      " The Thing I'm Most Afraid Of\n",
      " Anne of Green Gables\n",
      "The Paper Cowboy\n"
     ]
    }
   ],
   "source": [
    "#Gathering info from a user\n",
    "print(\"Design a Quiz\")\n",
    "print(\"Which book would you like to use?\")\n",
    "print (\"Here are your choices:\\n\")\n",
    "print(\"The Best Bad Luck I Ever Had \\n The Lions of Little Rock\\n The Paper Cowboy\\n The Thing I'm Most Afraid Of\\n Anne of Green Gables\")\n",
    "book = input()\n",
    "if book == \"The Best Bad Luck I Ever Had\":\n",
    "    narrator = \"Dit\"\n",
    "    text = \"Book_1\"\n",
    "elif book == \"The Lions of Little Rock\":\n",
    "    narrator = \"Marlee\"\n",
    "    text = \"Book_2\"\n",
    "elif book == \"The Paper Cowboy\":\n",
    "    narrator = \"Tommy\"\n",
    "    text = \"Book_3\"\n",
    "elif book == \"The Thing I'm Most Afraid Of\":\n",
    "    narrator = \"Becca\"\n",
    "    text = \"Book_4\"\n",
    "elif book == \"Anne of Green Gables\":\n",
    "    narrator = ''\n",
    "    text = \"Book_5\"\n",
    "else:\n",
    "    print(\"I'm sorry. That book is not in our database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a file dictionary\n",
    "#This assumes the files are broken down into chapters in a folder\n",
    "import os\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "#Creating the file dictionary\n",
    "# point this to the data directory\n",
    "direc = \"/Users/kristinlevine/Desktop/NLP_Project/\" + text \n",
    "ext = '.txt' # Select your file delimiter\n",
    "\n",
    "file_dict = {} # Create an empty dict\n",
    "\n",
    "# Select only files with the ext extension\n",
    "txt_files = [i for i in os.listdir(direc) if os.path.splitext(i)[1] == ext]\n",
    "\n",
    "# Iterate over your txt files\n",
    "for f in txt_files:\n",
    "    # Open them and assign them to file_dict\n",
    "    with open(os.path.join(direc,f)) as file_object:\n",
    "        file_dict[f] = file_object.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 53 chapters in this book.\n",
      "Which chapter would you like to analyze?\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", len(txt_files), 'chapters in this book.')\n",
    "print(\"Which chapter would you like to analyze?\")\n",
    "chapter = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To extract persons and locations\n",
    "def extract_entities(doc_id, doc_text):\n",
    "    analyzed_doc = nlp(doc_text)\n",
    "\n",
    "    doc_persons = {}\n",
    "    doc_locations = {}\n",
    "    \n",
    "    for entity in analyzed_doc.ents:\n",
    "        if entity.text.strip() != \"\" and entity.label_ == \"PERSON\":\n",
    "            prev_token = analyzed_doc[entity.start-1]\n",
    "            if prev_token.text in (\"Doc\", \"Dr.\", \"Mr.\", \"Mrs.\", 'Ms.'):\n",
    "                name = prev_token.text + ' '+ entity.text.strip()\n",
    "                if name not in doc_persons.keys():\n",
    "                    relevant_sentence = (doc_id, entity.sent.text)\n",
    "                    doc_persons[name] = list()\n",
    "                if name in doc_persons.keys():\n",
    "                    relevant_sentence = (doc_id, entity.sent.text)\n",
    "                    doc_persons[name].append(relevant_sentence)  \n",
    "            else:\n",
    "                if entity.text.strip() not in doc_persons.keys():\n",
    "                    relevant_sentence = (doc_id, entity.sent.text)\n",
    "                    doc_persons[entity.text.strip()] = list()\n",
    "                \n",
    "                if entity.text.strip() in doc_persons.keys():\n",
    "                    relevant_sentence = (doc_id, entity.sent.text)\n",
    "                    doc_persons[entity.text.strip()].append(relevant_sentence)  \n",
    "                \n",
    "    for entity in analyzed_doc.ents:\n",
    "        if entity.text.strip() != \"\" and (entity.label_ == 'LOC' or entity.label_ == \"GPE\"):\n",
    "            \n",
    "            if entity.text.strip() not in doc_locations.keys():\n",
    "                relevant_sentence = (doc_id, entity.sent.text)\n",
    "                doc_locations[entity.text.strip()] = list()\n",
    "                \n",
    "            if entity.text.strip() in doc_locations.keys():\n",
    "                relevant_sentence = (doc_id, entity.sent.text)\n",
    "                doc_locations[entity.text.strip()].append(relevant_sentence)  \n",
    "            \n",
    "    return doc_persons, doc_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_popular_entities(entity_dictionary):\n",
    "    \n",
    "    list_most_mentions = {}\n",
    "    \n",
    "    for entity in entity_dictionary:\n",
    "        x = []\n",
    "        for i in range(len(entity_dictionary[entity])):\n",
    "            x.append(len(entity_dictionary[entity][i]))\n",
    "        list_most_mentions[entity] = sum(x)\n",
    "        \n",
    "    # sort through the entities in the dictionary by the number of sentences\n",
    "    \n",
    "    return list_most_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sister Ann': 2, 'Mary Lou’s': 2, 'Mary Lou': 4, 'Mrs. Scully': 4, 'Model Railroader': 2, 'Dick Contino': 4, 'Mr. McKenzie': 2, 'Auld Lang Syne': 2, 'Dad': 2, 'Mom': 2, 'Pinky': 2}\n"
     ]
    }
   ],
   "source": [
    "a, b = extract_entities(chapter + '.txt', file_dict[chapter +'.txt'])\n",
    "tp = find_most_popular_entities(a)\n",
    "tl = find_most_popular_entities(b)\n",
    "print(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Persons\n",
    "if len(tp) > 0:\n",
    "    chapter_persons = pd.DataFrame.from_dict(tp, orient = 'index')\n",
    "    chapter_persons = chapter_persons.rename(columns = {0:'Count'})\n",
    "    chapter_persons = chapter_persons.sort_values(by=['Count'], ascending = False)\n",
    "#print(chapter_persons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Locations\n",
    "if len(tl) > 0:\n",
    "    chapter_locations = pd.DataFrame.from_dict(tl, orient = 'index')\n",
    "    chapter_locations = chapter_locations.rename(columns = {0:'Count'})\n",
    "    chapter_locations = chapter_locations.sort_values(by=['Count'], ascending = False)\n",
    "else:\n",
    "    chapter_locations = []\n",
    "#print(chapter_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_persons = {}\n",
    "combined_locations = {}\n",
    "\n",
    "for item in txt_files: \n",
    "    persons, locations = extract_entities(item, file_dict[item])\n",
    "\n",
    "#For Persons\n",
    "    for per in persons.keys():\n",
    "        if per not in combined_persons.keys():\n",
    "            combined_persons[per] = list()\n",
    "        \n",
    "        if per in combined_persons.keys():\n",
    "            combined_persons[per].append(persons.get(per))\n",
    "\n",
    "#For Locations\n",
    "    for loc in locations.keys():\n",
    "        if loc not in combined_locations.keys():\n",
    "            combined_locations[loc] = list()\n",
    "        \n",
    "        if loc in combined_locations.keys():\n",
    "            combined_locations[loc].append(locations.get(loc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mary Lou', 'Eddie', 'Mr. McKenzie', 'Mrs. Glazov', 'Sam', 'Dad', 'Pinky', 'Little Skinny', 'Skinny', 'Sister Ann', 'Mr. Sullivan', 'Luke', 'Peter', 'Mrs. Scully', 'Mom', 'Susie', 'Ma', 'Boots', 'Russo', 'Dr. Stanton', 'Mary Lou’s', 'Pa', 'Mrs. Sullivan', 'McKenzie', 'Gary Cooper', 'McCarthy', 'Catherine', 'yo-yos', 'Radulovich']\n"
     ]
    }
   ],
   "source": [
    "#Creating a List of the top characters in the book\n",
    "all_characters = find_most_popular_entities(combined_persons)\n",
    "char_in_book = pd.DataFrame.from_dict(all_characters, orient = 'index')\n",
    "char_in_book = char_in_book.rename(columns = {0:\"Count\"})\n",
    "char_in_book = char_in_book.sort_values(by=['Count'], ascending = False)\n",
    "main_char = char_in_book.head(30)\n",
    "people = main_char.index.tolist()\n",
    "#We also need to remove the narrator from this list because he/she can't talk to him/herself.\n",
    "words_remove = ['Moundville', \"Negra\"]\n",
    "if narrator in people:\n",
    "    people.remove(narrator)\n",
    "else: \n",
    "    pass\n",
    "\n",
    "for m in words_remove:\n",
    "    if m in people:\n",
    "        people.remove(m)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tivoli', 'Chicago', 'Kopeckys', 'Vienna', 'St. Joe', 'Rosenbergs', 'Hungary', 'the Soviet Union', 'Europe', 'Mindszenty', 'the United States', 'St. Joe’s', 'King', 'London', 'Slovak', 'U.S.', 'Mass']\n"
     ]
    }
   ],
   "source": [
    "#Creating a List of the top 20 locations in the book\n",
    "all_locations = find_most_popular_entities(combined_locations)\n",
    "loc_in_book = pd.DataFrame.from_dict(all_locations, orient = 'index')\n",
    "loc_in_book = loc_in_book.rename(columns = {0:\"Count\"})\n",
    "loc_in_book = loc_in_book.sort_values(by=['Count'], ascending = False)\n",
    "main_loc = loc_in_book.head(20)\n",
    "location = main_loc.index.tolist()\n",
    "words_to_remove = [\"ain’t\", 'Lover', \"Cordelia\", '☺', \"A.M.\", \"Curtis\", \"Negro\", \"Red\", \"a.k.a\", \"Busia\"]\n",
    "\n",
    "for i in people:\n",
    "    if i in location:\n",
    "        location.remove(i)\n",
    "    elif narrator in location:\n",
    "        location.remove(narrator)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "for j in words_to_remove:\n",
    "    if j in location:\n",
    "        location.remove(j)\n",
    "    else: \n",
    "        pass\n",
    "        \n",
    "print(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the characters/locations in each chapters against the main_character/locations list.\n",
    "#This makes sure we are asking about a fairly major character/location; not someone/someplace who is mentioned only once.\n",
    "\n",
    "chapter_characters = []\n",
    "for i in range(len(chapter_persons)):\n",
    "    for x in people:\n",
    "        if chapter_persons.index[i] == x:\n",
    "            chapter_characters.append(chapter_persons.index[i])\n",
    "        else:\n",
    "            pass    \n",
    "    \n",
    "chapter_loc = []\n",
    "for i in range(len(chapter_locations)):\n",
    "    for x in location:\n",
    "        if chapter_locations.index[i] == x:\n",
    "            chapter_loc.append(chapter_locations.index[i])\n",
    "        else:\n",
    "            pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mary Lou', 'Mrs. Scully', 'Sister Ann', 'Mary Lou’s', 'Mr. McKenzie', 'Dad', 'Mom', 'Pinky']\n",
      "['the Soviet Union', 'St. Joe', 'Hungary', 'Europe']\n"
     ]
    }
   ],
   "source": [
    "print(chapter_characters)\n",
    "print(chapter_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Paper Cowboy\n",
      "Chapter 13 : Questions for Discussion\n",
      "What did Tommy talk about with Mary Lou ?\n",
      "What did Tommy and Mrs. Scully discuss?\n",
      "What did Tommy say to Sister Ann ?\n",
      "Why is the Soviet Union mentioned in this chapter?\n"
     ]
    }
   ],
   "source": [
    "print(book)\n",
    "print(\"Chapter\", chapter, \": Questions for Discussion\")\n",
    "\n",
    "\n",
    "#This accounts for if the book has a narrator or not:\n",
    "if narrator == \"\":\n",
    "    narrator = chapter_characters[0]\n",
    "    i = 1\n",
    "else:\n",
    "    i = 0\n",
    "\n",
    "#This will print up to three character questions per chapter\n",
    "if len(chapter_characters) == 0:\n",
    "    pass\n",
    "elif len(chapter_characters) == 1:\n",
    "    print(\"What did\", narrator, \"talk about with\", chapter_characters[i], \"?\")\n",
    "elif len(chapter_characters) == 2:\n",
    "    print(\"What did\", narrator, \"talk about with\", chapter_characters[i], \"?\")\n",
    "    print(\"What did\", narrator, \"and\", chapter_characters[i+1], \"discuss?\")\n",
    "else:\n",
    "    print(\"What did\", narrator, \"talk about with\", chapter_characters[i], \"?\")\n",
    "    print(\"What did\", narrator, \"and\", chapter_characters[i+1], \"discuss?\")\n",
    "    print(\"What did\", narrator, \"say to\", chapter_characters[i+2], \"?\")\n",
    "    \n",
    "#This will print a location question:\n",
    "if len(chapter_loc) == 0:\n",
    "    pass\n",
    "else:\n",
    "    print(\"Why is\", chapter_loc[0], \"mentioned in this chapter?\")\n",
    "    \n",
    "# We'd print these questions, any vocab words, etc, to a pdf that the user could then print out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to figure out how to deal with characters with prefixes \"Mrs. Walker\" vs. \"Walker\"\n",
    "#We'd write these questions to a pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
